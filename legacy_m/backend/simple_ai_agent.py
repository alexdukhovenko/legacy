"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏—Å–ª–∞–º—Å–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
–ë–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è sentence-transformers (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
"""

import json
import re
import openai
import os
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import or_
from database import QuranVerse, Hadith, Commentary, VectorEmbedding, SystemConfig, OrthodoxText
from backend.confession_agents import ConfessionAgentFactory
import logging

logger = logging.getLogger(__name__)


class SimpleIslamicAIAgent:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏—Å–ª–∞–º—Å–∫–∏–º–∏ —Å–≤—è—â–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
    
    def __init__(self, db: Session):
        self.db = db
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API
        api_key = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
        self.openai_client = openai.OpenAI(api_key=api_key)
    
    def search_relevant_texts(self, query: str, user_confession: str = None, top_k: int = 2) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(query)
            
            results = []
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–∫–∞—Ç—å
            confession_filters = ['common']  # –û–±—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
            
            if user_confession:
                confession_filters.append(user_confession)
            
            # –ü–æ–∏—Å–∫ –≤ –∞—è—Ç–∞—Ö –ö–æ—Ä–∞–Ω–∞
            quran_query = self.db.query(QuranVerse)
            if confession_filters:
                # –í–∫–ª—é—á–∞–µ–º NULL –∑–Ω–∞—á–µ–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –±–µ–∑ –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏) + —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏
                quran_query = quran_query.filter(
                    or_(
                        QuranVerse.confession.in_(confession_filters),
                        QuranVerse.confession.is_(None)
                    )
                )
            
            quran_verses = quran_query.all()
            for verse in quran_verses:
                score = self._calculate_similarity_score(keywords, verse.translation_ru or "")
                if score > 0.1:  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    results.append({
                        'id': verse.id,
                        'source_type': 'quran',
                        'source_id': verse.id,
                        'text_chunk': verse.translation_ru or verse.arabic_text,
                        'similarity_score': score,
                        'content': {
                            'type': 'quran',
                            'surah_number': verse.surah_number,
                            'verse_number': verse.verse_number,
                            'arabic_text': verse.arabic_text,
                            'translation_ru': verse.translation_ru,
                            'theme': verse.theme,
                            'confession': verse.confession
                        }
                    })
            
            # –ü–æ–∏—Å–∫ –≤ —Ö–∞–¥–∏—Å–∞—Ö
            hadith_query = self.db.query(Hadith)
            if confession_filters:
                hadith_query = hadith_query.filter(
                    or_(
                        Hadith.confession.in_(confession_filters),
                        Hadith.confession.is_(None)
                    )
                )
            
            hadiths = hadith_query.all()
            for hadith in hadiths:
                score = self._calculate_similarity_score(keywords, hadith.translation_ru or "")
                if score > 0.1:
                    results.append({
                        'id': hadith.id,
                        'source_type': 'hadith',
                        'source_id': hadith.id,
                        'text_chunk': hadith.translation_ru or hadith.arabic_text,
                        'similarity_score': score,
                        'content': {
                            'type': 'hadith',
                            'collection': hadith.collection,
                            'hadith_number': hadith.hadith_number,
                            'arabic_text': hadith.arabic_text,
                            'translation_ru': hadith.translation_ru,
                            'narrator': hadith.narrator,
                            'grade': hadith.grade,
                            'topic': hadith.topic,
                            'confession': hadith.confession
                        }
                    })
            
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö
            commentary_query = self.db.query(Commentary)
            if confession_filters:
                commentary_query = commentary_query.filter(
                    or_(
                        Commentary.confession.in_(confession_filters),
                        Commentary.confession.is_(None)
                    )
                )
            
            commentaries = commentary_query.all()
            for commentary in commentaries:
                score = self._calculate_similarity_score(keywords, commentary.translation_ru or "")
                if score > 0.1:
                    results.append({
                        'id': commentary.id,
                        'source_type': 'commentary',
                        'source_id': commentary.id,
                        'text_chunk': commentary.translation_ru or commentary.arabic_text,
                        'similarity_score': score,
                        'content': {
                            'type': 'commentary',
                            'source': commentary.source,
                            'arabic_text': commentary.arabic_text,
                            'translation_ru': commentary.translation_ru,
                            'confession': commentary.confession
                        }
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = clean_text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫—Ç–æ', '—ç—Ç–æ', '—Ç–æ', '–∞', '–Ω–æ', '–∏–ª–∏', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '—á—Ç–æ–±—ã', '—á—Ç–æ–±—ã', '–º–µ–Ω—è', '–º–Ω–µ', '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–º—ã', '–≤—ã', '–æ–Ω–∏', '–≤—Å–µ', '–µ—Å–ª–∏', '–∏–ª–∏', '–Ω–æ', '–∞', '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '–ø—Ä–∏', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–æ–∫–æ–ª–æ', '–≤–æ–∑–ª–µ', '–±–ª–∏–∑', '–¥–∞–ª–µ–∫–æ', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞', '—Å—é–¥–∞', '–æ—Ç—Ç—É–¥–∞', '–æ—Ç—Å—é–¥–∞', '–∫–æ–≥–¥–∞', '—Ç–æ–≥–¥–∞', '—Å–µ–π—á–∞—Å', '—Ç–µ–ø–µ—Ä—å', '—É–∂–µ', '–µ—â–µ', '—Ç–æ–ª—å–∫–æ', '–ª–∏—à—å', '–¥–∞–∂–µ', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–µ—â–µ', '—É–∂–µ', '–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞', '–∏–Ω–æ–≥–¥–∞', '—á–∞—Å—Ç–æ', '—Ä–µ–¥–∫–æ', '–æ—á–µ–Ω—å', '—Å–ª–∏—à–∫–æ–º', '–¥–æ–≤–æ–ª—å–Ω–æ', '–≤–ø–æ–ª–Ω–µ', '—Å–æ–≤—Å–µ–º', '–ø–æ–ª–Ω–æ—Å—Ç—å—é', '—á–∞—Å—Ç–∏—á–Ω–æ', '–Ω–µ–º–Ω–æ–≥–æ', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–±–æ–ª—å—à–µ', '–º–µ–Ω—å—à–µ', '–ª—É—á—à–µ', '—Ö—É–∂–µ', '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–±—É–¥–µ—Ç', '–±—É–¥—É—Ç', '–º–æ–≥—É', '–º–æ–∂–µ—à—å', '–º–æ–∂–µ—Ç', '–º–æ–∂–µ–º', '–º–æ–∂–µ—Ç–µ', '–º–æ–≥—É—Ç', '—Ö–æ—á—É', '—Ö–æ—á–µ—à—å', '—Ö–æ—á–µ—Ç', '—Ö–æ—Ç–∏–º', '—Ö–æ—Ç–∏—Ç–µ', '—Ö–æ—Ç—è—Ç', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–∞', '–¥–æ–ª–∂–Ω–æ', '–¥–æ–ª–∂–Ω—ã', '–º–æ–∂–Ω–æ', '–Ω–µ–ª—å–∑—è', '–≤–æ–∑–º–æ–∂–Ω–æ', '–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ', '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–¥–∞', '–Ω–µ—Ç'}
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        synonyms = {
            '–ø–æ—Å—Ç': ['–ø–æ—Å—Ç–∏—Ç—å—Å—è', '–≥–æ–ª–æ–¥–∞—Ç—å', '–≤–æ–∑–¥–µ—Ä–∂–∞–Ω–∏–µ', '—Ä–∞–º–∞–¥–∞–Ω'],
            '–Ω–∞–º–∞–∑': ['–º–æ–ª–∏—Ç–≤–∞', '—Å–∞–ª—è—Ç', '–º–æ–ª–∏—Ç—å—Å—è'],
            '–¥—É–∞': ['–º–æ–ª—å–±–∞', '–ø—Ä–æ—Å—å–±–∞', '–º–æ–ª–∏—Ç—å—Å—è'],
            '–ø—Ä–µ—Ä–≤–∞—Ç—å': ['–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', '–ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å', '–±—Ä–æ—Å–∏—Ç—å', '–∑–∞–∫–æ–Ω—á–∏—Ç—å'],
            '—Å–æ–±–ª—é–¥–∞—Ç—å': ['–¥–µ—Ä–∂–∞—Ç—å', '–≤—ã–ø–æ–ª–Ω—è—Ç—å', '—Å–ª–µ–¥–æ–≤–∞—Ç—å'],
            '–¥—É—Ö–æ–≤–Ω–∏–∫': ['–∏–º–∞–º', '–º—É–ª–ª–∞', '—Å–≤—è—â–µ–Ω–Ω–∏–∫', '–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        extended_keywords = keywords.copy()
        for keyword in keywords:
            if keyword in synonyms:
                extended_keywords.extend(synonyms[keyword])
        
        return extended_keywords
    
    def _calculate_similarity_score(self, keywords: List[str], text: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        matches = 0
        total_weight = 0
        
        for keyword in keywords:
            total_weight += 1
            if keyword in text_lower:
                matches += 1
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                for word in text_lower.split():
                    if keyword in word or word in keyword:
                        matches += 0.5
                        break
        
        return matches / total_weight if total_weight > 0 else 0.0
    
    def generate_response(self, user_question: str, user_confession: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ñ–µ—Å—Å–∏–π"""
        try:
            logger.info(f"=== –ù–ê–ß–ê–õ–û –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–í–ï–¢–ê ===")
            logger.info(f"–í–æ–ø—Ä–æ—Å: {user_question}")
            logger.info(f"–ö–æ–Ω—Ñ–µ—Å—Å–∏—è: {user_confession}")
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–Ω—Ñ–µ—Å—Å–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
            if user_confession and user_confession in ['sunni', 'shia', 'orthodox']:
                try:
                    logger.info(f"–°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç –¥–ª—è –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏: {user_confession}")
                    agent = ConfessionAgentFactory.create_agent(user_confession, self.db)
                    logger.info(f"–ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ: {type(agent).__name__}")
                    
                    relevant_texts = agent.search_relevant_texts(user_question)
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(relevant_texts)}")
                    
                    if relevant_texts:
                        logger.info(f"–¢–∏–ø—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {[t.get('type', 'unknown') for t in relevant_texts]}")
                    
                    result = agent.generate_response(user_question, relevant_texts)
                    logger.info(f"–û—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞: {result.get('response', '')[:100]}...")
                    logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞: {len(result.get('sources', []))}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞–≥–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª –≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
                    if result and result.get('response'):
                        logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞ {user_confession}")
                        return result
                    else:
                        logger.warning(f"–ê–≥–µ–Ω—Ç {user_confession} –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞–≥–µ–Ω—Ç–µ –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏ {user_confession}: {e}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Ç–æ–ª—å–∫–æ –¥–ª—è –∏—Å–ª–∞–º—Å–∫–∏—Ö –∫–æ–Ω—Ñ–µ—Å—Å–∏–π
                    if user_confession in ['sunni', 'shia']:
                        logger.info(f"Fallback –¥–ª—è –∏—Å–ª–∞–º—Å–∫–æ–π –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏ {user_confession}")
                    else:
                        # –î–ª—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                        logger.info(f"Fallback –¥–ª—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–∏—è")
                        return {
                            'response': '–í –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–≤—è—â–µ–Ω–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º.',
                            'sources': [],
                            'confidence': 0.3
                        }
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ fallback
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥")
            return self._generate_standard_response(user_question, user_confession)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.',
                'sources': [],
                'confidence': 0.0
            }
    
    def _generate_standard_response(self, user_question: str, user_confession: str = None) -> Dict[str, Any]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (fallback –º–µ—Ç–æ–¥)"""
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏
            relevant_texts = self.search_relevant_texts(user_question, user_confession, top_k=2)
            
            if not relevant_texts:
                return {
                    'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É –∏–ª–∏ –∏–º–∞–º—É.',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
            # –ï—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π, —Å—á–∏—Ç–∞–µ–º –≤–æ–ø—Ä–æ—Å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
            max_score = max([text['similarity_score'] for text in relevant_texts]) if relevant_texts else 0
            if max_score < 0.5:  # –ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                return {
                    'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É –∏–ª–∏ –∏–º–∞–º—É.',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —è–≤–Ω–æ –Ω–µ —Å–≤—è–∑–∞–Ω —Å —Ä–µ–ª–∏–≥–∏–µ–π, –Ω–µ –æ—Ç–≤–µ—á–∞–µ–º
            non_religious_keywords = ['–∫—Ä—ã–º', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–±–æ—Ä—â', '–≥–æ—Ç–æ–≤–∏—Ç—å', '—Ä–µ—Ü–µ–ø—Ç', '—Ñ—É—Ç–±–æ–ª', '—Å–ø–æ—Ä—Ç', '–∫–∏–Ω–æ', '–º—É–∑—ã–∫–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ']
            question_lower = user_question.lower()
            if any(keyword in question_lower for keyword in non_religious_keywords):
                return {
                    'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫ –∏ –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–µ–ª–∏–≥–∏–µ–π –∏ –¥—É—Ö–æ–≤–Ω–æ—Å—Ç—å—é. –î–ª—è –¥—Ä—É–≥–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º.',
                    'sources': [],
                    'confidence': 0.0
                }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è OpenAI
            context = self._prepare_context_for_openai(relevant_texts)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è OpenAI
            system_prompt = """–¢—ã –∏—Å–ª–∞–º—Å–∫–∏–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
2. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
3. "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" + —á–µ–ª–æ–≤–µ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

–ü–†–ò–ú–ï–†:
"–í –ö–æ—Ä–∞–Ω–µ, —Å—É—Ä–∞ 2, –∞—è—Ç 255 –≥–æ–≤–æ—Ä–∏—Ç—Å—è –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –º–æ–ª–∏—Ç–≤—ã.

–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –ú–æ–ª–∏—Ç–≤–∞ —É–∫—Ä–µ–ø–ª—è–µ—Ç –≤–µ—Ä—É –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º—å—é."

–ü–†–ê–í–ò–õ–ê:
- –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ù–ï –ø–∏—à–∏ —Ç–æ–ª–∫–æ–≤–∞–Ω–∏—è –∞—Å-–°–∞–∞–¥–∏
- –ë–£–î–¨ –ö–†–ê–¢–ö–ò–ú (–º–∞–∫—Å–∏–º—É–º 100 —Å–ª–æ–≤)
- –ù–ï –æ—Ç–≤–µ—á–∞–π –Ω–∞ –Ω–µ–∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"""

            user_prompt = f"""–í–æ–ø—Ä–æ—Å: {user_question}

–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {context}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –ø–æ –ø—Ä–∏–º–µ—Ä—É –≤—ã—à–µ. –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã!"""

            # –ó–∞–ø—Ä–æ—Å –∫ OpenAI
            response = self.openai_client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_output_tokens=1500,  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                temperature=0.3  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            )
            
            ai_response = response.choices[0].message.content.strip()
            logger.info(f"üîç –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI: {ai_response}")
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∑–∞–º–µ–Ω—è–µ–º "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" –Ω–∞ "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü
            if "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" in ai_response:
                ai_response = ai_response.replace("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
                logger.info(f"üîÑ –ó–∞–º–µ–Ω–∏–ª '–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:' –Ω–∞ '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:'")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü –ø–µ—Ä–µ–¥ "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" in ai_response:
                parts = ai_response.split("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:", 1)
                if len(parts) == 2:
                    main_part = parts[0].rstrip()
                    appendix_part = parts[1]
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∞–±–∑–∞—Ü –ø–µ—Ä–µ–¥ "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:"
                    if not main_part.endswith("\n\n"):
                        ai_response = f"{main_part}\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:{appendix_part}"
                        logger.info(f"üîÑ –î–æ–±–∞–≤–∏–ª –∞–±–∑–∞—Ü –ø–µ—Ä–µ–¥ '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:'")
                    else:
                        ai_response = f"{main_part}–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:{appendix_part}"
                        logger.info(f"üîÑ –ê–±–∑–∞—Ü —É–∂–µ –µ—Å—Ç—å –ø–µ—Ä–µ–¥ '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:'")
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            logger.info(f"üîç –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(ai_response)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ–º –ª—é–±–æ–π –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–Ω–µ–µ 300 —Å–∏–º–≤–æ–ª–æ–≤
            if len(ai_response) > 300:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
                short_response = ai_response[:200] + "..."
                
                # –î–æ–±–∞–≤–ª—è–µ–º "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" not in short_response:
                    short_response += "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
                
                ai_response = short_response
                logger.info(f"üîÑ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û —Å–æ–∫—Ä–∞—Ç–∏–ª –æ—Ç–≤–µ—Ç –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            brief_sources = []
            for text in relevant_texts:
                content = text['content']
                if content['type'] == 'quran':
                    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤–µ—Ä—Å–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
                    brief_content = {
                        'type': 'quran',
                        'id': content.get('id'),
                        'surah_number': content['surah_number'],
                        'verse_number': content['verse_number'],
                        'arabic_text': content.get('arabic_text', '')[:50] + '...' if content.get('arabic_text') else '',
                        'translation_ru': content.get('translation_ru', '')[:100] + '...' if content.get('translation_ru') else '',
                        'theme': content.get('theme', '–æ–±—â–∏–π'),
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
                        'full_arabic_text': content.get('arabic_text', ''),
                        'full_translation_ru': content.get('translation_ru', ''),
                        'full_commentary': content.get('commentary', '')
                    }
                    brief_sources.append({
                        'type': text['content']['type'],
                        'similarity_score': text['similarity_score'],
                        'content': brief_content
                    })
            
            return {
                'response': ai_response,
                'sources': brief_sources,
                'confidence': max([text['similarity_score'] for text in relevant_texts]) if relevant_texts else 0.0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            return self._generate_simple_fallback(user_question, relevant_texts)
    
    def _prepare_context_for_openai(self, texts: List[Dict]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è OpenAI —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã"""
        context_parts = []
        
        for text in texts:
            content = text['content']
            
            if content['type'] == 'quran':
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
                translation = content.get('translation_ru', '')
                if len(translation) > 150:
                    translation = translation[:150] + "..."
                
                confession_info = f" ({content.get('confession', '–æ–±—â–∏–π')})" if content.get('confession') else ""
                context_parts.append(
                    f"–ö–æ—Ä–∞–Ω{confession_info}, —Å—É—Ä–∞ {content['surah_number']}, –∞—è—Ç {content['verse_number']}: "
                    f"{translation}"
                )
                
            elif content['type'] == 'hadith':
                translation = content.get('translation_ru', '')
                if len(translation) > 150:
                    translation = translation[:150] + "..."
                
                confession_info = f" ({content.get('confession', '–æ–±—â–∏–π')})" if content.get('confession') else ""
                context_parts.append(
                    f"–•–∞–¥–∏—Å{confession_info}, —Å–±–æ—Ä–Ω–∏–∫ {content.get('collection', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}, "
                    f"–Ω–æ–º–µ—Ä {content.get('hadith_number', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}: {translation}"
                )
                
            elif content['type'] == 'commentary':
                translation = content.get('translation_ru', '')
                if len(translation) > 150:
                    translation = translation[:150] + "..."
                
                confession_info = f" ({content.get('confession', '–æ–±—â–∏–π')})" if content.get('confession') else ""
                context_parts.append(
                    f"–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π{confession_info}, –∏—Å—Ç–æ—á–Ω–∏–∫ {content.get('source', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}: "
                    f"{translation}"
                )
        
        return "\n\n".join(context_parts)
    
    def _generate_simple_fallback(self, question: str, texts: List[Dict]) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –æ—Ç–≤–µ—Ç –±–µ–∑ OpenAI"""
        if not texts:
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É –∏–ª–∏ –∏–º–∞–º—É.',
                'sources': [],
                'confidence': 0.0
            }
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        first_text = texts[0]['content']
        if first_text['type'] == 'quran':
            response = f"–í –ö–æ—Ä–∞–Ω–µ, —Å—É—Ä–∞ {first_text['surah_number']}, –∞—è—Ç {first_text['verse_number']} –≥–æ–≤–æ—Ä–∏—Ç—Å—è: {first_text['translation_ru']}"
        
        return {
            'response': response,
            'sources': [{
                'type': text['content']['type'],
                'similarity_score': text['similarity_score'],
                'content': text['content']
            } for text in texts],
            'confidence': max([text['similarity_score'] for text in texts]) if texts else 0.0
        }
    
    def _form_response(self, question: str, texts: List[Dict]) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            response_parts = [
                "–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤:",
                ""
            ]
            
            for text in texts:
                content = text['content']
                if content['type'] == 'quran':
                    response_parts.append(f"–ö–æ—Ä–∞–Ω, —Å—É—Ä–∞ {content['surah_number']}, –∞—è—Ç {content['verse_number']}:")
                    response_parts.append(f"{content['translation_ru']}")
                    if content.get('arabic_text'):
                        response_parts.append(f"–ê—Ä–∞–±—Å–∫–∏–π —Ç–µ–∫—Å—Ç: {content['arabic_text']}")
                    response_parts.append("")
            
            response_parts.extend([
                "–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ª–∏—á–Ω–æ–≥–æ –¥—É—Ö–æ–≤–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É –∏–ª–∏ –∏–º–∞–º—É –≤ –≤–∞—à–µ–π –º–µ—Å—Ç–Ω–æ–π –º–µ—á–µ—Ç–∏."
            ])
            
            return "\n".join(response_parts)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –¥—É—Ö–æ–≤–Ω–∏–∫—É."
    
    def add_text_to_database(self, text: str, source_type: str, source_id: int, chunk_size: int = 512):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)"""
        try:
            # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å
            vector_embedding = VectorEmbedding(
                source_type=source_type,
                source_id=source_id,
                text_chunk=text,
                embedding_vector="",  # –ü—É—Å—Ç–æ–π –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                chunk_index=0
            )
            
            self.db.add(vector_embedding)
            self.db.commit()
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è {source_type}:{source_id}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            self.db.rollback()
