#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π AI –∞–≥–µ–Ω—Ç —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∏ –æ—Ç–≤–µ—Ç–∞–º–∏
"""

import os
import logging
import time
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import or_, and_
import openai
from openai import OpenAI

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥–µ–ª–µ–π
from database.models import QuranVerse, Hadith, Commentary, OrthodoxText, OrthodoxDocument

logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class EnhancedAIAgent:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π AI –∞–≥–µ–Ω—Ç —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self, confession: str, db: Session):
        self.confession = confession
        self.db = db
        self.confession_name = confession
        
    def generate_quality_response(self, question: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        start_time = time.time()
        
        try:
            logger.info(f"ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è {self.confession}")
            
            # –®–∞–≥ 1: –ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            relevant_texts = self._deep_search_relevant_texts(question)
            logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(relevant_texts)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
            
            # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            analyzed_texts = self._analyze_and_rank_sources(question, relevant_texts)
            logger.info(f"üéØ –û—Ç–æ–±—Ä–∞–Ω–æ {len(analyzed_texts)} –ª—É—á—à–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = self._build_rich_context(analyzed_texts)
            
            # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å GPT-4
            response = self._generate_thoughtful_response(question, context, analyzed_texts)
            
            # –®–∞–≥ 5: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
            final_response = self._postprocess_response(response, analyzed_texts)
            
            processing_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            return final_response
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return self._fallback_response(question)
    
    def _deep_search_relevant_texts(self, question: str) -> List[Dict[str, Any]]:
        """–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        results = []
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_results = self._semantic_search(question)
        results.extend(semantic_results)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_results = self._keyword_search(question)
        results.extend(keyword_results)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        thematic_results = self._thematic_search(question)
        results.extend(thematic_results)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_results = self._remove_duplicates(results)
        
        return unique_results
    
    def _semantic_search(self, question: str) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI embeddings"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = self._get_embedding(question)
            
            # –ò—â–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö
            if self.confession == 'orthodox':
                texts = self.db.query(OrthodoxText).filter(
                    OrthodoxText.confession == 'orthodox'
                ).limit(50).all()
            elif self.confession == 'sunni':
                quran_verses = self.db.query(QuranVerse).filter(
                    or_(QuranVerse.confession == 'sunni', QuranVerse.confession.is_(None))
                ).limit(30).all()
                hadiths = self.db.query(Hadith).filter(
                    Hadith.confession == 'sunni'
                ).limit(20).all()
                texts = list(quran_verses) + list(hadiths)
            elif self.confession == 'shia':
                quran_verses = self.db.query(QuranVerse).filter(
                    or_(QuranVerse.confession == 'shia', QuranVerse.confession.is_(None))
                ).limit(30).all()
                hadiths = self.db.query(Hadith).filter(
                    Hadith.confession == 'shia'
                ).limit(20).all()
                texts = list(quran_verses) + list(hadiths)
            else:
                texts = []
            
            results = []
            for text in texts:
                text_content = self._extract_text_content(text)
                if text_content:
                    text_embedding = self._get_embedding(text_content)
                    similarity = self._cosine_similarity(question_embedding, text_embedding)
                    
                    if similarity > 0.1:  # –ü–æ—Ä–æ–≥ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        results.append({
                            'text': text,
                            'content': text_content,
                            'similarity': similarity,
                            'type': 'semantic'
                        })
            
            return results
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _keyword_search(self, question: str) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        keywords = self._extract_keywords(question)
        results = []
        
        if self.confession == 'orthodox':
            texts = self.db.query(OrthodoxText).filter(
                OrthodoxText.confession == 'orthodox'
            ).limit(30).all()
        elif self.confession == 'sunni':
            quran_verses = self.db.query(QuranVerse).filter(
                or_(QuranVerse.confession == 'sunni', QuranVerse.confession.is_(None))
            ).limit(20).all()
            hadiths = self.db.query(Hadith).filter(
                Hadith.confession == 'sunni'
            ).limit(15).all()
            texts = list(quran_verses) + list(hadiths)
        elif self.confession == 'shia':
            quran_verses = self.db.query(QuranVerse).filter(
                or_(QuranVerse.confession == 'shia', QuranVerse.confession.is_(None))
            ).limit(20).all()
            hadiths = self.db.query(Hadith).filter(
                Hadith.confession == 'shia'
            ).limit(15).all()
            texts = list(quran_verses) + list(hadiths)
        else:
            texts = []
        
        for text in texts:
            text_content = self._extract_text_content(text)
            if text_content:
                score = self._calculate_keyword_score(keywords, text_content)
                if score > 0.1:
                    results.append({
                        'text': text,
                        'content': text_content,
                        'similarity': score,
                        'type': 'keyword'
                    })
        
        return results
    
    def _thematic_search(self, question: str) -> List[Dict[str, Any]]:
        """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º"""
        themes = self._identify_themes(question)
        results = []
        
        # –°–ª–æ–≤–∞—Ä—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
        theme_keywords = {
            '–º–æ–ª–∏—Ç–≤–∞': ['–º–æ–ª–∏—Ç–≤–∞', '–º–æ–ª–∏—Ç—å—Å—è', '–¥—É–∞', '–Ω–∞–º–∞–∑', '—Å–∞–ª—è—Ç', '–±–æ–≥–æ—Å–ª—É–∂–µ–Ω–∏–µ', '–º–æ–ª–µ–Ω–∏–µ'],
            '–≥—Ä–µ—Ö': ['–≥—Ä–µ—Ö', '–≥—Ä–µ—à–∏—Ç—å', '–ø–æ–∫–∞—è–Ω–∏–µ', '–ø—Ä–æ—â–µ–Ω–∏–µ', '–∏—Å–∫—É–ø–ª–µ–Ω–∏–µ', '–≤–∏–Ω–∞'],
            '—Å–µ–º—å—è': ['—Å–µ–º—å—è', '—Ä–æ–¥–∏—Ç–µ–ª–∏', '–¥–µ—Ç–∏', '–±—Ä–∞—Ç', '—Å–µ—Å—Ç—Ä–∞', '—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏', '–±–ª–∏–∑–∫–∏–µ'],
            '–ª—é–±–æ–≤—å': ['–ª—é–±–æ–≤—å', '–ª—é–±–∏—Ç—å', '–º–∏–ª–æ—Å–µ—Ä–¥–∏–µ', '—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ', '–¥–æ–±—Ä–æ—Ç–∞', '–∑–∞–±–æ—Ç–∞'],
            '–≤–µ—Ä–∞': ['–≤–µ—Ä–∞', '–≤–µ—Ä–∏—Ç—å', '–¥–æ–≤–µ—Ä–∏–µ', '—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '—É–±–µ–∂–¥–µ–Ω–∏–µ', '–≤–µ—Ä–æ–≤–∞–Ω–∏–µ'],
            '—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ': ['—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ', '–±–æ–ª—å', '—Å–∫–æ—Ä–±—å', '–≥–æ—Ä–µ', '—Ç—Ä—É–¥–Ω–æ—Å—Ç–∏', '–ø—Ä–æ–±–ª–µ–º—ã'],
            '–≥–Ω–µ–≤': ['–≥–Ω–µ–≤', '–∑–ª–æ—Å—Ç—å', '—è—Ä–æ—Å—Ç—å', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ', '–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ', '—Å—Å–æ—Ä–∞'],
            '–ø—Ä–æ—â–µ–Ω–∏–µ': ['–ø—Ä–æ—â–µ–Ω–∏–µ', '–ø—Ä–æ—â–∞—Ç—å', '–º–∏–ª–æ—Å—Ç—å', '—Å–Ω–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ', '—Ç–µ—Ä–ø–µ–Ω–∏–µ']
        }
        
        if self.confession == 'orthodox':
            texts = self.db.query(OrthodoxText).filter(
                OrthodoxText.confession == 'orthodox'
            ).limit(20).all()
        elif self.confession == 'sunni':
            quran_verses = self.db.query(QuranVerse).filter(
                or_(QuranVerse.confession == 'sunni', QuranVerse.confession.is_(None))
            ).limit(15).all()
            hadiths = self.db.query(Hadith).filter(
                Hadith.confession == 'sunni'
            ).limit(10).all()
            texts = list(quran_verses) + list(hadiths)
        elif self.confession == 'shia':
            quran_verses = self.db.query(QuranVerse).filter(
                or_(QuranVerse.confession == 'shia', QuranVerse.confession.is_(None))
            ).limit(15).all()
            hadiths = self.db.query(Hadith).filter(
                Hadith.confession == 'shia'
            ).limit(10).all()
            texts = list(quran_verses) + list(hadiths)
        else:
            texts = []
        
        for text in texts:
            text_content = self._extract_text_content(text)
            if text_content:
                score = self._calculate_thematic_score(themes, theme_keywords, text_content)
                if score > 0.1:
                    results.append({
                        'text': text,
                        'content': text_content,
                        'similarity': score,
                        'type': 'thematic'
                    })
        
        return results
    
    def _analyze_and_rank_sources(self, question: str, texts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        semantic_texts = [t for t in texts if t['type'] == 'semantic']
        keyword_texts = [t for t in texts if t['type'] == 'keyword']
        thematic_texts = [t for t in texts if t['type'] == 'thematic']
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        weighted_results = []
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for text in semantic_texts:
            weighted_results.append({
                **text,
                'weighted_score': text['similarity'] * 1.0
            })
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ - —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for text in keyword_texts:
            weighted_results.append({
                **text,
                'weighted_score': text['similarity'] * 0.7
            })
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ - –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for text in thematic_texts:
            weighted_results.append({
                **text,
                'weighted_score': text['similarity'] * 0.5
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–º—É score
        weighted_results.sort(key=lambda x: x['weighted_score'], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return weighted_results[:5]
    
    def _build_rich_context(self, texts: List[Dict[str, Any]]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –±–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        context_parts = []
        
        for i, text_data in enumerate(texts, 1):
            text = text_data['text']
            content = text_data['content']
            
            if hasattr(text, 'surah_number'):  # –ö–æ—Ä–∞–Ω
                context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: –ö–æ—Ä–∞–Ω, —Å—É—Ä–∞ {text.surah_number}, –∞—è—Ç {text.verse_number}\n{content}")
            elif hasattr(text, 'collection'):  # –•–∞–¥–∏—Å
                context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: –•–∞–¥–∏—Å –∏–∑ {text.collection}\n{content}")
            elif hasattr(text, 'book_name'):  # –ü—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–π —Ç–µ–∫—Å—Ç
                context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {text.book_name}\n{content}")
            else:
                context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_thoughtful_response(self, question: str, context: str, sources: List[Dict[str, Any]]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–¥—É–º—á–∏–≤—ã–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT-4"""
        
        system_prompt = self._get_enhanced_system_prompt()
        
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
2. –î–∞–π –∫—Ä–∞—Ç–∫–∏–π, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
3. –î–æ–±–∞–≤—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
4. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
5. –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
6. –ë—É–¥—å —á–µ–ª–æ–≤–µ—á–Ω—ã–º –∏ –ø–æ–Ω–∏–º–∞—é—â–∏–º

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
[–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞]

–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç]"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=400,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._fallback_response(question)
    
    def _get_enhanced_system_prompt(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
        if self.confession == 'orthodox':
            return """–¢—ã –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω–æ–≥–æ —Ö—Ä–∏—Å—Ç–∏–∞–Ω—Å—Ç–≤–∞.

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º
- –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
- –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
- –ë—É–¥—å –ø–æ–Ω–∏–º–∞—é—â–∏–º –∏ –º—É–¥—Ä—ã–º"""
        
        elif self.confession == 'sunni':
            return """–¢—ã —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–π –∏—Å–ª–∞–º—Å–∫–∏–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–∑–∏—Ü–∏–∏ —Å—É–Ω–Ω–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞.

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–ö–æ—Ä–∞–Ω, —Ö–∞–¥–∏—Å—ã)
- –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º
- –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
- –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
- –ë—É–¥—å –ø–æ–Ω–∏–º–∞—é—â–∏–º –∏ –º—É–¥—Ä—ã–º"""
        
        elif self.confession == 'shia':
            return """–¢—ã —à–∏–∏—Ç—Å–∫–∏–π –∏—Å–ª–∞–º—Å–∫–∏–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–∑–∏—Ü–∏–∏ —à–∏–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞.

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —à–∏–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–ö–æ—Ä–∞–Ω, —Ö–∞–¥–∏—Å—ã –∏–∑ –∞–ª—å-–ö–∞—Ñ–∏)
- –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º
- –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
- –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
- –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
- –ë—É–¥—å –ø–æ–Ω–∏–º–∞—é—â–∏–º –∏ –º—É–¥—Ä—ã–º"""
        
        return "–¢—ã –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –º—É–¥—Ä–æ –∏ –ø–æ–Ω–∏–º–∞—é—â–µ."
    
    def _postprocess_response(self, response: str, sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        # –ó–∞–º–µ–Ω—è–µ–º "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" –Ω–∞ "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:"
        if "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" in response:
            response = response.replace("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø–µ—Ä–µ–¥ "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:"
        if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" in response and "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" not in response:
            response = response.replace("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:", "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        brief_sources = []
        for source_data in sources:
            text = source_data['text']
            if hasattr(text, 'surah_number'):  # –ö–æ—Ä–∞–Ω
                brief_sources.append({
                    'type': 'quran',
                    'surah': text.surah_number,
                    'verse': text.verse_number,
                    'text': text.translation_ru or text.arabic_text or "",
                    'full_text': text.translation_ru or text.arabic_text or ""
                })
            elif hasattr(text, 'collection'):  # –•–∞–¥–∏—Å
                brief_sources.append({
                    'type': 'hadith',
                    'collection': text.collection,
                    'number': getattr(text, 'hadith_number', getattr(text, 'number', '')),
                    'text': text.translation_ru or text.arabic_text or "",
                    'full_text': text.translation_ru or text.arabic_text or ""
                })
            elif hasattr(text, 'book_name'):  # –ü—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–π —Ç–µ–∫—Å—Ç
                brief_sources.append({
                    'type': 'orthodox',
                    'book': text.book_name,
                    'author': getattr(text, 'author', ''),
                    'text': text.translation_ru or text.original_text or "",
                    'full_text': text.translation_ru or text.original_text or ""
                })
        
        return {
            'response': response,
            'sources': brief_sources,
            'confidence': min(len(sources) * 0.2, 1.0)
        }
    
    def _fallback_response(self, question: str) -> Dict[str, Any]:
        """Fallback –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        if self.confession == 'orthodox':
            return {
                'response': '–í –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–≤—è—â–µ–Ω–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º.',
                'sources': [],
                'confidence': 0.3
            }
        elif self.confession == 'sunni':
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –î–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –º–µ—Å—Ç–Ω–æ–º—É –∏–º–∞–º—É –∏–ª–∏ —É—á–µ–Ω–æ–º—É.',
                'sources': [],
                'confidence': 0.3
            }
        elif self.confession == 'shia':
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —à–∏–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –î–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –º–µ—Å—Ç–Ω–æ–º—É –∞—è—Ç–æ–ª–ª–µ –∏–ª–∏ —É—á–µ–Ω–æ–º—É.',
                'sources': [],
                'confidence': 0.3
            }
        else:
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.',
                'sources': [],
                'confidence': 0.0
            }
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è embedding: {e}")
            return []
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        if not a or not b or len(a) != len(b):
            return 0.0
        
        import numpy as np
        a_np = np.array(a)
        b_np = np.array(b)
        
        dot_product = np.dot(a_np, b_np)
        norm_a = np.linalg.norm(a_np)
        norm_b = np.linalg.norm(b_np)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot_product / (norm_a * norm_b)
    
    def _extract_text_content(self, text) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ –æ–±—ä–µ–∫—Ç–∞"""
        if hasattr(text, 'translation_ru') and text.translation_ru:
            return text.translation_ru
        elif hasattr(text, 'arabic_text') and text.arabic_text:
            return text.arabic_text
        elif hasattr(text, 'original_text') and text.original_text:
            return text.original_text
        elif hasattr(text, 'text_russian') and text.text_russian:
            return text.text_russian
        else:
            return ""
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        import re
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = clean_text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '—á–µ–π', '—á—å—è', '—á—å–µ', '–º–æ—è', '–º–æ–π', '–º–æ–µ', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ–µ', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ'}
        
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return keywords[:10]  # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _calculate_keyword_score(self, keywords: List[str], text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç score –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        if not keywords:
            return 0.0
        
        text_lower = text.lower()
        matches = sum(1 for keyword in keywords if keyword in text_lower)
        return matches / len(keywords)
    
    def _identify_themes(self, question: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –≤ –≤–æ–ø—Ä–æ—Å–µ"""
        question_lower = question.lower()
        themes = []
        
        theme_keywords = {
            '–º–æ–ª–∏—Ç–≤–∞': ['–º–æ–ª–∏—Ç–≤–∞', '–º–æ–ª–∏—Ç—å—Å—è', '–¥—É–∞', '–Ω–∞–º–∞–∑', '—Å–∞–ª—è—Ç'],
            '–≥—Ä–µ—Ö': ['–≥—Ä–µ—Ö', '–≥—Ä–µ—à–∏—Ç—å', '–ø–æ–∫–∞—è–Ω–∏–µ', '–ø—Ä–æ—â–µ–Ω–∏–µ'],
            '—Å–µ–º—å—è': ['—Å–µ–º—å—è', '—Ä–æ–¥–∏—Ç–µ–ª–∏', '–¥–µ—Ç–∏', '–±—Ä–∞—Ç', '—Å–µ—Å—Ç—Ä–∞'],
            '–ª—é–±–æ–≤—å': ['–ª—é–±–æ–≤—å', '–ª—é–±–∏—Ç—å', '–º–∏–ª–æ—Å–µ—Ä–¥–∏–µ', '—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ'],
            '–≤–µ—Ä–∞': ['–≤–µ—Ä–∞', '–≤–µ—Ä–∏—Ç—å', '–¥–æ–≤–µ—Ä–∏–µ', '—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å'],
            '—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ': ['—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ', '–±–æ–ª—å', '—Å–∫–æ—Ä–±—å', '–≥–æ—Ä–µ'],
            '–≥–Ω–µ–≤': ['–≥–Ω–µ–≤', '–∑–ª–æ—Å—Ç—å', '—è—Ä–æ—Å—Ç—å', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ'],
            '–ø—Ä–æ—â–µ–Ω–∏–µ': ['–ø—Ä–æ—â–µ–Ω–∏–µ', '–ø—Ä–æ—â–∞—Ç—å', '–º–∏–ª–æ—Å—Ç—å', '—Ç–µ—Ä–ø–µ–Ω–∏–µ']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                themes.append(theme)
        
        return themes
    
    def _calculate_thematic_score(self, themes: List[str], theme_keywords: Dict[str, List[str]], text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π score"""
        if not themes:
            return 0.0
        
        text_lower = text.lower()
        score = 0.0
        
        for theme in themes:
            if theme in theme_keywords:
                for keyword in theme_keywords[theme]:
                    if keyword in text_lower:
                        score += 0.1
        
        return min(score, 1.0)
    
    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        seen = set()
        unique_results = []
        
        for result in results:
            text_id = getattr(result['text'], 'id', None)
            if text_id and text_id not in seen:
                seen.add(text_id)
                unique_results.append(result)
        
        return unique_results
