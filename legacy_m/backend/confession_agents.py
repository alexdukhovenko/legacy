#!/usr/bin/env python3
"""
–û—Ç–¥–µ–ª—å–Ω—ã–µ AI –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏
–ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –æ–±—É—á–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö —Å–≤–æ–µ–π –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏ –∏ –∏–º–µ–µ—Ç —Å–∏—Å—Ç–µ–º—É –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–∏
"""

import os
import logging
import openai
import numpy as np
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import or_, and_
from database.models import QuranVerse, Hadith, Commentary, OrthodoxText, OrthodoxDocument
from .simple_ai_provider import simple_ai_provider
from .simple_fallback import simple_fallback
from .enhanced_ai_agent import EnhancedAIAgent

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=openai.api_key)

class BaseConfessionAgent:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ñ–µ—Å—Å–∏–π"""
    
    def __init__(self, confession: str, db: Session):
        self.confession = confession
        self.db = db
    
        self.system_prompt = self._get_system_prompt()
        
    def _get_system_prompt(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏"""
        raise NotImplementedError
    
    def search_relevant_texts(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        raise NotImplementedError
    
    def generate_response(self, question: str, relevant_texts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        raise NotImplementedError
    
    def verify_response(self, question: str, response: str, sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å"""
        # –£–ø—Ä–æ—â–∞–µ–º –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if not sources:
            return {
                "is_accurate": False,
                "confidence": 0.0,
                "issues": ["–ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"],
                "recommendation": "–¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
            }
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
        if len(response) > 300:
            return {
                "is_accurate": False,
                "confidence": 0.3,
                "issues": ["–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π"],
                "recommendation": "–°–æ–∫—Ä–∞—Ç–∏—Ç—å –æ—Ç–≤–µ—Ç"
            }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –æ—Ç–≤–µ—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π - —Å—á–∏—Ç–∞–µ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–º
        return {
            "is_accurate": True,
            "confidence": 0.8,
            "issues": [],
            "recommendation": "–û—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"
        }
    
    def _calculate_similarity_score(self, question: str, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        if not text:
            return 0.0
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=[question, text]
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            question_embedding = response.data[0].embedding
            text_embedding = response.data[1].embedding
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            question_vector = np.array(question_embedding)
            text_vector = np.array(text_embedding)
            
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            dot_product = np.dot(question_vector, text_vector)
            norm_question = np.linalg.norm(question_vector)
            norm_text = np.linalg.norm(text_vector)
            
            if norm_question == 0 or norm_text == 0:
                return 0.0
            
            similarity = dot_product / (norm_question * norm_text)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            return self._fallback_similarity_score(question, text)
    
    def _fallback_similarity_score(self, question: str, text: str) -> float:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if not text:
            return 0.0
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        if not question_words or not text_words:
            return 0.0
        
        # Jaccard similarity
        intersection = len(question_words.intersection(text_words))
        union = len(question_words.union(text_words))
        
        base_score = intersection / union if union > 0 else 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å']
        for word in important_words:
            if word in question_clean and word in text_clean:
                base_score += 0.3
        
        return min(base_score, 1.0)

class SunniAgent(BaseConfessionAgent):
    """AI –∞–≥–µ–Ω—Ç –¥–ª—è —Å—É–Ω–Ω–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞"""
    
    def _get_system_prompt(self) -> str:
        return """–¢—ã —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–π –∏—Å–ª–∞–º—Å–∫–∏–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–∑–∏—Ü–∏–∏ —Å—É–Ω–Ω–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞.

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
        1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        2. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        3. "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" + —á–µ–ª–æ–≤–µ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        –ü–†–ê–í–ò–õ–ê:
        - –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–ö–æ—Ä–∞–Ω, —Ö–∞–¥–∏—Å—ã –∏–∑ –°–∞—Ö–∏—Ö –∞–ª—å-–ë—É—Ö–∞—Ä–∏, –°–∞—Ö–∏—Ö –ú—É—Å–ª–∏–º, –∏ —Ç.–¥.)
        - –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        - –ë–£–î–¨ –ö–†–ê–¢–ö–ò–ú (–º–∞–∫—Å–∏–º—É–º 100 —Å–ª–æ–≤)
        - –ù–ï –æ—Ç–≤–µ—á–∞–π –Ω–∞ –Ω–µ–∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        - –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
    
    def search_relevant_texts(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        logger.info(f"üîç SunniAgent: –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
        
        # –ü–æ–∏—Å–∫ –≤ –ö–æ—Ä–∞–Ω–µ (—Ç–æ–ª—å–∫–æ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        quran_query = self.db.query(QuranVerse).filter(
            or_(
                QuranVerse.confession == 'sunni',
                QuranVerse.confession.is_(None)
            )
        )
        
        quran_count = quran_query.count()
        logger.info(f"üìñ SunniAgent: –ù–∞–π–¥–µ–Ω–æ {quran_count} –∞—è—Ç–æ–≤ –ö–æ—Ä–∞–Ω–∞ –¥–ª—è —Å—É–Ω–Ω–∏—Ç–æ–≤")
        
        # –ü–æ–∏—Å–∫ –≤ —Ö–∞–¥–∏—Å–∞—Ö (—Ç–æ–ª—å–∫–æ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–µ)
        hadith_query = self.db.query(Hadith).filter(
            Hadith.confession == 'sunni'
        )
        
        hadith_count = hadith_query.count()
        logger.info(f"üìú SunniAgent: –ù–∞–π–¥–µ–Ω–æ {hadith_count} —Ö–∞–¥–∏—Å–æ–≤ –¥–ª—è —Å—É–Ω–Ω–∏—Ç–æ–≤")
        
        # –ü–æ–∏—Å–∫ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö (—Ç–æ–ª—å–∫–æ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏–µ)
        commentary_query = self.db.query(Commentary).filter(
            Commentary.confession == 'sunni'
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—è—Ç—ã –ö–æ—Ä–∞–Ω–∞
        for verse in quran_query.limit(limit * 5):  # –ë–µ—Ä–µ–º –µ—â–µ –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–±–æ—Ä–∞
            score = self._calculate_similarity_score(question, verse.translation_ru or "")
            if score > 0.01:  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                results.append({
                    'type': 'quran',
                    'content': {
                        'id': verse.id,
                        'type': 'quran',
                        'surah_number': verse.surah_number,
                        'verse_number': verse.verse_number,
                        'arabic_text': verse.arabic_text,
                        'translation_ru': verse.translation_ru,
                        'commentary': verse.commentary,
                        'theme': verse.theme
                    },
                    'similarity_score': score
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞–¥–∏—Å—ã
        for hadith in hadith_query.limit(limit * 3):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö —Ö–∞–¥–∏—Å–æ–≤
            score = self._calculate_similarity_score(question, hadith.translation_ru or "")
            if score > 0.000001:  # –£–õ–¨–¢–†–ê –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∏—Å–ª–∞–º—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
                results.append({
                    'type': 'hadith',
                    'content': {
                        'id': hadith.id,
                        'type': 'hadith',
                        'collection': hadith.collection,
                        'hadith_number': hadith.hadith_number,
                        'arabic_text': hadith.arabic_text,
                        'translation_ru': hadith.translation_ru,
                        'narrator': hadith.narrator,
                        'grade': hadith.grade,
                        'topic': hadith.topic,
                        'commentary': hadith.commentary
                    },
                    'similarity_score': score
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ SunniAgent
        if len(results) == 0:
            logger.warning(f"üö® SunniAgent: –ù–ï –ù–ê–ô–î–ï–ù–û –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
            logger.warning(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ similarity_score: 0.000001")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∏—Ö scores
            all_results = []
            for hadith in hadith_query.limit(5):
                score = self._calculate_similarity_score(question, hadith.translation_ru or "")
                all_results.append(f"–•–∞–¥–∏—Å {hadith.id}: score={score:.8f}")
            logger.warning(f"üìä –ü–µ—Ä–≤—ã–µ 5 —Ö–∞–¥–∏—Å–æ–≤ —Å scores: {all_results}")
        else:
            scores = [f"{r['similarity_score']:.8f}" for r in results[:3]]
            logger.info(f"‚úÖ SunniAgent: –ù–∞–π–¥–µ–Ω–æ {len(results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å scores: {scores}")
        
        return results[:limit]
    
    def generate_response(self, question: str, relevant_texts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        if not relevant_texts:
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å—É–Ω–Ω–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.',
                'sources': [],
                'confidence': 0.0
            }
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self._prepare_context(relevant_texts)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

        –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {context}

        –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –ø–æ –ø—Ä–∏–º–µ—Ä—É –≤—ã—à–µ. –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã!"""
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å fallback
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            try:
                response_text = simple_ai_provider.generate_response(messages, max_tokens=800)
                logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –æ—Ç AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø–æ–ª—É—á–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback
                fallback_result = simple_fallback.generate_response(question, self.confession_name, relevant_texts)
                return fallback_result
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            if "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" in response_text:
                response_text = response_text.replace("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" in response_text and "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" not in response_text:
                response_text = response_text.replace("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:", "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            # –ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
            verification = self.verify_response(question, response_text, relevant_texts)
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –æ—Ç–≤–µ—Ç
            if verification['confidence'] < 0.7:
                response_text = f"–í —Å—É–Ω–Ω–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º."
            
            return {
                'response': response_text,
                'sources': relevant_texts,
                'confidence': verification['confidence'],
                'verification': verification
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.',
                'sources': [],
                'confidence': 0.0
            }
    
    def _prepare_context(self, texts: List[Dict[str, Any]]) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI"""
        context_parts = []
        
        for text in texts:
            content = text['content']
            if content['type'] == 'quran':
                context_parts.append(f"–ö–æ—Ä–∞–Ω, —Å—É—Ä–∞ {content['surah_number']}, –∞—è—Ç {content['verse_number']}: {content['translation_ru'][:150]}...")
            elif content['type'] == 'hadith':
                context_parts.append(f"–•–∞–¥–∏—Å –∏–∑ {content['collection']}: {content['translation_ru'][:150]}...")
        
        return "\n".join(context_parts)
    
    def _calculate_similarity(self, question: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_words = question_words.intersection(text_words)
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return True
        
        return len(common_words) >= 1  # –°–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –¥–æ 1 —Å–ª–æ–≤–∞
    
    def _calculate_similarity_score(self, question: str, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not text:
            return 0.0
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        if not question_words or not text_words:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å', '–ø—Ä–æ—Ä–æ–∫', '–º—É—Ö–∞–º–º–∞–¥']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return 0.8  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        
        # Jaccard similarity
        intersection = len(question_words.intersection(text_words))
        union = len(question_words.union(text_words))
        
        base_score = intersection / union if union > 0 else 0.0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for q_word in question_words:
            for t_word in text_words:
                if len(q_word) > 3 and len(t_word) > 3:
                    if q_word in t_word or t_word in q_word:
                        base_score += 0.2
        
        return min(base_score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º–æ–º 1.0


class ShiaAgent(BaseConfessionAgent):
    """AI –∞–≥–µ–Ω—Ç –¥–ª—è —à–∏–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞"""
    
    def _get_system_prompt(self) -> str:
        return """–¢—ã —à–∏–∏—Ç—Å–∫–∏–π –∏—Å–ª–∞–º—Å–∫–∏–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–∑–∏—Ü–∏–∏ —à–∏–∏—Ç—Å–∫–æ–≥–æ –∏—Å–ª–∞–º–∞.

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
        1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        2. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        3. "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" + —á–µ–ª–æ–≤–µ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        –ü–†–ê–í–ò–õ–ê:
        - –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —à–∏–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–ö–æ—Ä–∞–Ω, —Ö–∞–¥–∏—Å—ã –∏–∑ –∞–ª—å-–ö–∞—Ñ–∏, –∏ —Ç.–¥.)
        - –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        - –ë–£–î–¨ –ö–†–ê–¢–ö–ò–ú (–º–∞–∫—Å–∏–º—É–º 100 —Å–ª–æ–≤)
        - –ù–ï –æ—Ç–≤–µ—á–∞–π –Ω–∞ –Ω–µ–∏—Å–ª–∞–º—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        - –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
    
    def search_relevant_texts(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ —à–∏–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        logger.info(f"üîç ShiaAgent: –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
        
        # –ü–æ–∏—Å–∫ –≤ –ö–æ—Ä–∞–Ω–µ (—Ç–æ–ª—å–∫–æ —à–∏–∏—Ç—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        quran_query = self.db.query(QuranVerse).filter(
            or_(
                QuranVerse.confession == 'shia',
                QuranVerse.confession.is_(None)
            )
        )
        
        quran_count = quran_query.count()
        logger.info(f"üìñ ShiaAgent: –ù–∞–π–¥–µ–Ω–æ {quran_count} –∞—è—Ç–æ–≤ –ö–æ—Ä–∞–Ω–∞ –¥–ª—è —à–∏–∏—Ç–æ–≤")
        
        # –ü–æ–∏—Å–∫ –≤ —Ö–∞–¥–∏—Å–∞—Ö (—Ç–æ–ª—å–∫–æ —à–∏–∏—Ç—Å–∫–∏–µ)
        hadith_query = self.db.query(Hadith).filter(
            Hadith.confession == 'shia'
        )
        
        hadith_count = hadith_query.count()
        logger.info(f"üìú ShiaAgent: –ù–∞–π–¥–µ–Ω–æ {hadith_count} —Ö–∞–¥–∏—Å–æ–≤ –¥–ª—è —à–∏–∏—Ç–æ–≤")
        
        # –ü–æ–∏—Å–∫ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö (—Ç–æ–ª—å–∫–æ —à–∏–∏—Ç—Å–∫–∏–µ)
        commentary_query = self.db.query(Commentary).filter(
            Commentary.confession == 'shia'
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ SunniAgent)
        results = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—è—Ç—ã –ö–æ—Ä–∞–Ω–∞
        for verse in quran_query.limit(limit * 5):  # –ë–µ—Ä–µ–º –µ—â–µ –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–±–æ—Ä–∞
            score = self._calculate_similarity_score(question, verse.translation_ru or "")
            if score > 0.01:  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                results.append({
                    'type': 'quran',
                    'content': {
                        'id': verse.id,
                        'type': 'quran',
                        'surah_number': verse.surah_number,
                        'verse_number': verse.verse_number,
                        'arabic_text': verse.arabic_text,
                        'translation_ru': verse.translation_ru,
                        'commentary': verse.commentary,
                        'theme': verse.theme
                    },
                    'similarity_score': score
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞–¥–∏—Å—ã
        for hadith in hadith_query.limit(limit * 3):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö —Ö–∞–¥–∏—Å–æ–≤
            score = self._calculate_similarity_score(question, hadith.translation_ru or "")
            if score > 0.000001:  # –£–õ–¨–¢–†–ê –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∏—Å–ª–∞–º—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
                results.append({
                    'type': 'hadith',
                    'content': {
                        'id': hadith.id,
                        'type': 'hadith',
                        'collection': hadith.collection,
                        'hadith_number': hadith.hadith_number,
                        'arabic_text': hadith.arabic_text,
                        'translation_ru': hadith.translation_ru,
                        'narrator': hadith.narrator,
                        'grade': hadith.grade,
                        'topic': hadith.topic,
                        'commentary': hadith.commentary
                    },
                    'similarity_score': score
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ ShiaAgent
        if len(results) == 0:
            logger.warning(f"üö® ShiaAgent: –ù–ï –ù–ê–ô–î–ï–ù–û –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
            logger.warning(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ similarity_score: 0.000001")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∏—Ö scores
            all_results = []
            for hadith in hadith_query.limit(5):
                score = self._calculate_similarity_score(question, hadith.translation_ru or "")
                all_results.append(f"–•–∞–¥–∏—Å {hadith.id}: score={score:.8f}")
            logger.warning(f"üìä –ü–µ—Ä–≤—ã–µ 5 —Ö–∞–¥–∏—Å–æ–≤ —Å scores: {all_results}")
        else:
            scores = [f"{r['similarity_score']:.8f}" for r in results[:3]]
            logger.info(f"‚úÖ ShiaAgent: –ù–∞–π–¥–µ–Ω–æ {len(results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å scores: {scores}")
        
        return results[:limit]
    
    def generate_response(self, question: str, relevant_texts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–æ–π (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ SunniAgent)"""
        if not relevant_texts:
            logger.warning(f"üö® ShiaAgent: generate_response –ø–æ–ª—É—á–∏–ª 0 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —à–∏–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.',
                'sources': [],
                'confidence': 0.0
            }
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self._prepare_context(relevant_texts)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

        –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {context}

        –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –ø–æ –ø—Ä–∏–º–µ—Ä—É –≤—ã—à–µ. –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã!"""
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å fallback
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            try:
                response_text = simple_ai_provider.generate_response(messages, max_tokens=800)
                logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –æ—Ç AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø–æ–ª—É—á–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback
                fallback_result = simple_fallback.generate_response(question, self.confession_name, relevant_texts)
                return fallback_result
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            if "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" in response_text:
                response_text = response_text.replace("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" in response_text and "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" not in response_text:
                response_text = response_text.replace("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:", "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            # –ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
            verification = self.verify_response(question, response_text, relevant_texts)
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –æ—Ç–≤–µ—Ç
            if verification['confidence'] < 0.7:
                response_text = f"–í —à–∏–∏—Ç—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∑–Ω–∞—é—â–µ–º—É –¥—É—Ö–æ–≤–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º."
            
            return {
                'response': response_text,
                'sources': relevant_texts,
                'confidence': verification['confidence'],
                'verification': verification
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.',
                'sources': [],
                'confidence': 0.0
            }
    
    def _prepare_context(self, texts: List[Dict[str, Any]]) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI"""
        context_parts = []
        
        for text in texts:
            content = text['content']
            if content['type'] == 'quran':
                context_parts.append(f"–ö–æ—Ä–∞–Ω, —Å—É—Ä–∞ {content['surah_number']}, –∞—è—Ç {content['verse_number']}: {content['translation_ru'][:150]}...")
            elif content['type'] == 'hadith':
                context_parts.append(f"–•–∞–¥–∏—Å –∏–∑ {content['collection']}: {content['translation_ru'][:150]}...")
        
        return "\n".join(context_parts)
    
    def _calculate_similarity(self, question: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_words = question_words.intersection(text_words)
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return True
        
        return len(common_words) >= 1  # –°–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –¥–æ 1 —Å–ª–æ–≤–∞
    
    def _calculate_similarity_score(self, question: str, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not text:
            return 0.0
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        if not question_words or not text_words:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å', '–ø—Ä–æ—Ä–æ–∫', '–º—É—Ö–∞–º–º–∞–¥']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return 0.8  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        
        # Jaccard similarity
        intersection = len(question_words.intersection(text_words))
        union = len(question_words.union(text_words))
        
        base_score = intersection / union if union > 0 else 0.0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for q_word in question_words:
            for t_word in text_words:
                if len(q_word) > 3 and len(t_word) > 3:
                    if q_word in t_word or t_word in q_word:
                        base_score += 0.2
        
        return min(base_score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º–æ–º 1.0


class OrthodoxAgent(BaseConfessionAgent):
    """AI –∞–≥–µ–Ω—Ç –¥–ª—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–∏—è"""
    
    def __init__(self, confession: str, db: Session):
        super().__init__(confession, db)
        self.confession_name = "orthodox"
    
    def _get_system_prompt(self) -> str:
        return """–¢—ã –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–π –¥—É—Ö–æ–≤–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–∞–≤–æ—Å–ª–∞–≤–∏–µ–º.

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°–¢–†–û–ì–û):
        1. –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        2. –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        3. "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" + —á–µ–ª–æ–≤–µ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        –ü–†–ê–í–ò–õ–ê:
        - –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–ë–∏–±–ª–∏—è, —Å–≤—è—Ç–æ–æ—Ç–µ—á–µ—Å–∫–∏–µ —Ç—Ä—É–¥—ã, –¥–æ–≥–º–∞—Ç–∏–∫–∞)
        - –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        - –ë–£–î–¨ –ö–†–ê–¢–ö–ò–ú (–º–∞–∫—Å–∏–º—É–º 100 —Å–ª–æ–≤)
        - –ù–ï –æ—Ç–≤–µ—á–∞–π –Ω–∞ –Ω–µ—Ä–µ–ª–∏–≥–∏–æ–∑–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        - –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        - –ù–ï —É–ø–æ–º–∏–Ω–∞–π –∏—Å–ª–∞–º –∏–ª–∏ –º—É—Å—É–ª—å–º–∞–Ω—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        - –û—Ç–≤–µ—á–∞–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω–æ–≥–æ —Ö—Ä–∏—Å—Ç–∏–∞–Ω—Å—Ç–≤–∞
        - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –ë–æ–≥–µ, –æ—Ç–≤–µ—á–∞–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω–æ–≥–æ —É—á–µ–Ω–∏—è –æ –¢—Ä–æ–∏—Ü–µ
        - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –¢—Ä–æ–∏—Ü–µ, –æ–±—ä—è—Å–Ω–∏ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¢—Ä–æ–∏—Ü—ã"""
    
    def search_relevant_texts(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        # –ü–æ–∏—Å–∫ –≤ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        orthodox_query = self.db.query(OrthodoxText).filter(
            OrthodoxText.confession == 'orthodox'
        )
        
        results = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        for text in orthodox_query.limit(limit * 10):  # –ë–µ—Ä–µ–º –µ—â–µ –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–±–æ—Ä–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –ø—Ä–∞–≤–æ—Å–ª–∞–≤–∏—è
            score = self._fallback_similarity_score(question, text.translation_ru or "")
            if score > 0.1:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–æ–∏—Å–∫
                results.append({
                    'type': 'orthodox',
                    'content': {
                        'id': text.id,
                        'type': 'orthodox',
                        'source_type': text.source_type,
                        'book_name': text.book_name,
                        'author': text.author,
                        'chapter_number': text.chapter_number,
                        'verse_number': text.verse_number,
                        'original_text': text.original_text,
                        'translation_ru': text.translation_ru,
                        'commentary': text.commentary,
                        'theme': text.theme
                    },
                    'similarity_score': score
                })
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –±–µ—Ä–µ–º –ª—é–±—ã–µ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if not results:
            logger.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            for text in orthodox_query.limit(3):
                results.append({
                    'type': 'orthodox',
                    'content': {
                        'id': text.id,
                        'type': 'orthodox',
                        'source_type': text.source_type,
                        'book_name': text.book_name,
                        'author': text.author,
                        'chapter_number': text.chapter_number,
                        'verse_number': text.verse_number,
                        'original_text': text.original_text,
                        'translation_ru': text.translation_ru,
                        'commentary': text.commentary,
                        'theme': text.theme
                    },
                    'similarity_score': 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è fallback
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        return results[:limit]
    
    def generate_response(self, question: str, relevant_texts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        # –í—Å–µ–≥–¥–∞ –∏—â–µ–º –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
        if not relevant_texts:
            logger.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∏—â–µ–º –ª—é–±—ã–µ –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
            orthodox_query = self.db.query(OrthodoxText).filter(
                OrthodoxText.confession == 'orthodox'
            ).limit(3)
            
            relevant_texts = []
            for text in orthodox_query:
                relevant_texts.append({
                    'type': 'orthodox',
                    'content': {
                        'id': text.id,
                        'type': 'orthodox',
                        'source_type': text.source_type,
                        'book_name': text.book_name,
                        'author': text.author,
                        'chapter_number': text.chapter_number,
                        'verse_number': text.verse_number,
                        'original_text': text.original_text,
                        'translation_ru': text.translation_ru,
                        'commentary': text.commentary,
                        'theme': text.theme
                    },
                    'similarity_score': 0.1
                })
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â–∏–π –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã–π –æ—Ç–≤–µ—Ç
        if not relevant_texts:
            return {
                'response': '–í –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–≤—è—â–µ–Ω–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º.',
                'sources': [],
                'confidence': 0.3
            }
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self._prepare_context(relevant_texts)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

        –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {context}

        –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –ø–æ –ø—Ä–∏–º–µ—Ä—É –≤—ã—à–µ. –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã!"""
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å fallback
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            try:
                response_text = simple_ai_provider.generate_response(messages, max_tokens=800)
                logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –æ—Ç AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –ø–æ–ª—É—á–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
                logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback
                fallback_result = simple_fallback.generate_response(question, self.confession_name, relevant_texts)
                return fallback_result
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            if "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:" in response_text:
                response_text = response_text.replace("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            if "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" in response_text and "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:" not in response_text:
                response_text = response_text.replace("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:", "\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:")
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –Ω–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –æ—Ç–≤–µ—Ç
            if not relevant_texts:
                response_text = f"–í –ø—Ä–∞–≤–æ—Å–ª–∞–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É, –Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–≤—è—â–µ–Ω–Ω–∏–∫—É.\n\n–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: –≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Å–≤—è—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å –¥—É—Ö–æ–≤–Ω—ã–º –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º."
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            brief_sources = []
            for text in relevant_texts:
                content = text['content']
                if content['type'] == 'orthodox':
                    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤–µ—Ä—Å–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
                    brief_content = {
                        'type': 'orthodox',
                        'id': content.get('id'),
                        'book_name': content.get('book_name', ''),
                        'author': content.get('author', ''),
                        'chapter_number': content.get('chapter_number'),
                        'verse_number': content.get('verse_number'),
                        'translation_ru': content.get('translation_ru', '')[:100] + '...' if content.get('translation_ru') else '',
                        'theme': content.get('theme', '–æ–±—â–∏–π'),
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
                        'full_translation_ru': content.get('translation_ru', ''),
                        'full_commentary': content.get('commentary', '')
                    }
                    brief_sources.append({
                        'type': text['content']['type'],
                        'similarity_score': text['similarity_score'],
                        'content': brief_content
                    })
            
            return {
                'response': response_text,
                'sources': brief_sources,
                'confidence': 0.8 if brief_sources else 0.3
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'response': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.',
                'sources': [],
                'confidence': 0.0
            }
    
    def _prepare_context(self, texts: List[Dict[str, Any]]) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI"""
        context_parts = []
        
        for text in texts:
            content = text['content']
            if content['type'] == 'orthodox':
                source_info = f"{content['book_name']}"
                if content['author']:
                    source_info += f" ({content['author']})"
                if content['chapter_number'] and content['verse_number']:
                    source_info += f", –≥–ª–∞–≤–∞ {content['chapter_number']}, —Å—Ç–∏—Ö {content['verse_number']}"
                elif content['chapter_number']:
                    source_info += f", –≥–ª–∞–≤–∞ {content['chapter_number']}"
                
                context_parts.append(f"{source_info}: {content['translation_ru'][:150]}...")
        
        return "\n".join(context_parts)
    
    def _calculate_similarity(self, question: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_words = question_words.intersection(text_words)
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return True
        
        return len(common_words) >= 1  # –°–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –¥–æ 1 —Å–ª–æ–≤–∞
    
    def _calculate_similarity_score(self, question: str, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not text:
            return 0.0
        
        # –û—á–∏—â–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re
        question_clean = re.sub(r'[^\w\s]', ' ', question.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        question_words = set(question_clean.split())
        text_words = set(text_clean.split())
        
        if not question_words or not text_words:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        important_words = ['–±–æ–≥', '–∞–ª–ª–∞—Ö', '–º–æ–ª–∏—Ç–≤–∞', '–≤–µ—Ä–∞', '–∏—Å–ª–∞–º', '–∫–æ—Ä–∞–Ω', '—Ö–∞–¥–∏—Å', '–ø—Ä–æ—Ä–æ–∫', '–º—É—Ö–∞–º–º–∞–¥']
        for word in important_words:
            if word in question_clean and word in text_clean:
                return 0.8  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        
        # Jaccard similarity
        intersection = len(question_words.intersection(text_words))
        union = len(question_words.union(text_words))
        
        base_score = intersection / union if union > 0 else 0.0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for q_word in question_words:
            for t_word in text_words:
                if len(q_word) > 3 and len(t_word) > 3:
                    if q_word in t_word or t_word in q_word:
                        base_score += 0.2
        
        return min(base_score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º–æ–º 1.0


class ConfessionAgentFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ñ–µ—Å—Å–∏–π"""
    
    @staticmethod
    def create_agent(confession: str, db: Session) -> BaseConfessionAgent:
        """–°–æ–∑–¥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–µ—Å—Å–∏–∏"""
        if confession == 'sunni':
            return SunniAgent(confession, db)
        elif confession == 'shia':
            return ShiaAgent(confession, db)
        elif confession == 'orthodox':
            return OrthodoxAgent(confession, db)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –∫–æ–Ω—Ñ–µ—Å—Å–∏—è: {confession}")
